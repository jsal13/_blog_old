{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title:  \"Making Your Data Flow With Sklearn Pipelines\"\n",
    "date:   2022-01-06\n",
    "\n",
    "description: The basics of Sklearn pipelines.\n",
    "categories: python sklearn datascience data\n",
    "\n",
    "excerpt: Sklearn's [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) are an elegant way to organize your modeling workflow.  It also provides an \"at-a-glance\" picture of what is going into the current model &mdash; something your future self will thank you for when you read that notebook back in six months.\n",
    "\n",
    "classes: wide\n",
    "\n",
    "header:\n",
    "  overlay_filter: rgba(0, 146, 202, 0.8)\n",
    "  overlay_image: /assets/images/title_pipeline.jpg\n",
    "  caption: \"Photo Credit: [**GordonJ86**](https://commons.wikimedia.org/wiki/File:Orange_and_Yellow_HDPE_Pipe.jpg)\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Your Data Flow With Sklearn Pipelines\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sklearn's [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) are an elegant way to organize your modeling workflow.  It also provides an \"at-a-glance\" picture of what is going into the current model &mdash; something your future self will thank you for when you read that notebook back in six months.\n",
    "\n",
    "## Getting Toy Data To Play With\n",
    "\n",
    "Let's import all the libraries we're working with (don't worry if you don't know what some of these do, we'll get to it!) and get some toy data to work with.  We'll be working with the cute [Penguins](https://github.com/allisonhorst/palmerpenguins) dataset which ``seaborn`` can load.\n",
    "\n",
    "**Note that I will be emphasizing type hints and style quite a bit!**\n",
    "\n",
    "**Goal**: We'll try to predict the sex, given the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mypy users, as of 2022-01-08, seaborn does not use typing.\n",
    "# We have to wrap the load in `pd.DataFrame` to make mypy\n",
    "# understand that it is a dataframe.`\n",
    "# See: https://github.com/mwaskom/seaborn/issues/2212\n",
    "\n",
    "df = pd.DataFrame(sns.load_dataset(\"penguins\"))  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's do some quick EDA to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    Male  \n",
       "1       3800.0  Female  \n",
       "2       3250.0  Female  \n",
       "3          NaN     NaN  \n",
       "4       3450.0  Female  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>342.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>342.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>43.921930</td>\n",
       "      <td>17.151170</td>\n",
       "      <td>200.915205</td>\n",
       "      <td>4201.754386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.459584</td>\n",
       "      <td>1.974793</td>\n",
       "      <td>14.061714</td>\n",
       "      <td>801.954536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>32.100000</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>2700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.225000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>3550.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>44.450000</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>4050.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.500000</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>4750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.600000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>6300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n",
       "count      342.000000     342.000000         342.000000   342.000000\n",
       "mean        43.921930      17.151170         200.915205  4201.754386\n",
       "std          5.459584       1.974793          14.061714   801.954536\n",
       "min         32.100000      13.100000         172.000000  2700.000000\n",
       "25%         39.225000      15.600000         190.000000  3550.000000\n",
       "50%         44.450000      17.300000         197.000000  4050.000000\n",
       "75%         48.500000      18.700000         213.000000  4750.000000\n",
       "max         59.600000      21.500000         231.000000  6300.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species               0\n",
       "island                0\n",
       "bill_length_mm        2\n",
       "bill_depth_mm         2\n",
       "flipper_length_mm     2\n",
       "body_mass_g           2\n",
       "sex                  11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the missing ``sex`` values, let's drop those rows for now, since we're trying to predict on ``sex``.  \n",
    "\n",
    "If we were to simply do \n",
    "```python\n",
    "df.dropna(subset=[\"sex\"], inplace=True)\n",
    "``` \n",
    "in a cell, we might forget that we applied this and get messed up down the line!  Because Jupyter Notebooks are pretty easy to mess up when you've got code in a bunch of different cells, we're going to make a data import function that does basic importing and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_penguin_data() -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Get and clean ``Penguins`` data.\n",
    "\n",
    "    Loads ``Penguins`` data, removes rows with null values for ``sex``.\n",
    "    Returns (df_features, df_target) as a tuple of dataframes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pd.DataFrame, pd.DataFrame]\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(sns.load_dataset(\"penguins\"))  # type: ignore\n",
    "    df.dropna(subset=[\"sex\"], inplace=True)\n",
    "\n",
    "    # Transform Male/Female into 0/1.\n",
    "    targets: pd.Series = df[\"sex\"].apply(\n",
    "        lambda x: 0 if x == \"Male\" else 1\n",
    "    )  # type: ignore\n",
    "\n",
    "    return (df.drop(\"sex\", axis=1), targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's look at our numeric data.  There's a few things to do:\n",
    " \n",
    "- We'd like to impute on the missing values,\n",
    "- We'd like to scale these down a bit so everything is nice and normalized.\n",
    "\n",
    "Let's use a ``Pipeline`` to do this.\n",
    "\n",
    "A ``Pipeline``will take a list of 2-tuples ``(name, transform)`` where a ``transform`` in Sklean is defined as anything which has implemented the ``fit``/``transform`` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_numeric = Pipeline(\n",
    "    [\n",
    "        (\"impute_w_mean\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scale_normal\", StandardScaler()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note here that the name of the first step of the pipeline is ``impute_w_mean`` and the associated transform is ``SimpleImputer``.  Similarly, ``scale_normal`` is associated to ``StandardScaler``.\n",
    "\n",
    "For kicks, let's run this through some data and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.30930734],\n",
       "       [-0.16366342],\n",
       "       [-0.16366342],\n",
       "       [ 0.        ],\n",
       "       [ 2.12762443],\n",
       "       [ 0.98198051],\n",
       "       [-1.30930734],\n",
       "       [-0.16366342],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running some fake data through ``pipeline_numeric`` for fun.\n",
    "fake_data = np.array([1, 2, 2, np.nan, 4, 3, 1, 2, np.nan]).reshape(-1, 1)\n",
    "pipeline_numeric.fit_transform(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting!  We see here that this replaced our N/A values with whatever the mean was, then normalized our data which sent the mean to 0.  Cool.\n",
    "\n",
    "\n",
    "What about the categorical data?  Can we do anything with that?  Since there are only a few islands (3) and a few species (3), we might try ``OneHotEncoder`` and see what we get from that.  Let's make a similar pipeline, imputing with the most frequent value if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_categorical = Pipeline(\n",
    "    [\n",
    "        (\"impute_w_most_frequent\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"one_hot_encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this one on some fake data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running some fake data through ``pipeline_categorical`` for fun.\n",
    "fake_data = np.array([\"a\", \"a\", \"b\", np.nan, np.nan], dtype=object).reshape(-1, 1)\n",
    "pipeline_categorical.fit_transform(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might be a bit harder to tell, but this imputed the missing values as \"a\", and then converted \"a\" and \"b\" to ``[1, 0]`` and ``[0, 1]`` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Putting It All Together\n",
    "\n",
    "So far, we've made our numeric and categorical pipelines for the loaded data. We need to tell Sklearn what pipeline each column should go into.  This is where ``ColumnTransformer`` comes in.  This time, we pass a list of 3-tuples in representing ``(name, pipeline, column names to use)``.  \n",
    "\n",
    "Our preprocessing code, excluding the helper function we made, should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All preprocessing code, excluding helper functions.\n",
    "\n",
    "pipeline_numeric = Pipeline(\n",
    "    [\n",
    "        (\"impute_w_mean\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scale_normal\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_categorical = Pipeline(\n",
    "    [\n",
    "        (\"impute_w_most_frequent\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"one_hot_encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "categorical_cols = [\"species\", \"island\"]\n",
    "\n",
    "preprocessing_transformer = ColumnTransformer(\n",
    "    [\n",
    "        (\"numeric\", pipeline_numeric, numeric_cols),\n",
    "        (\"categorical\", pipeline_categorical, categorical_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Model\n",
    "\n",
    "Let's make a simple model for this data.  A Random Forest might be a nice one, let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our classifier has a ``fit``/``transform`` method, it can also be pipelined.  Let's take our _entire preprocessing transformer_ and make that the first step, then push that into the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_model_pipeline = Pipeline(\n",
    "    [(\"preprocessing\", preprocessing_transformer), (\"random_forest_classifier\", rf_clf)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Train\n",
    "\n",
    "At this point, we'll break our original data into a training and test set and pass the training set through our pipeline.  Then we'll evaluate how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.9454545454545454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.9811320754716981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.9122807017543859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.9454545454545454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        value\n",
       "accuracy   0.9454545454545454\n",
       "precision  0.9811320754716981\n",
       "recall     0.9122807017543859\n",
       "f1         0.9454545454545454"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the Data.\n",
    "\n",
    "df_features, df_target = get_and_clean_penguin_data()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_features, df_target, test_size=0.33, random_state=1234\n",
    ")\n",
    "\n",
    "pmp = preprocess_model_pipeline.fit(x_train, y_train)\n",
    "\n",
    "# Predict!\n",
    "y_predicted = pmp.predict(x_test)\n",
    "\n",
    "# Score!\n",
    "scores = np.array(\n",
    "    [\n",
    "        (\"accuracy\", accuracy_score(y_test, y_predicted)),\n",
    "        (\"precision\", precision_score(y_test, y_predicted)),\n",
    "        (\"recall\", recall_score(y_test, y_predicted)),\n",
    "        (\"f1\", f1_score(y_test, y_predicted)),\n",
    "    ]\n",
    ")\n",
    "df_scores = pd.DataFrame(scores[:, 1], index=scores[:, 0], columns=[\"value\"])\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad!\n",
    "\n",
    "\n",
    "## The Complete Code.\n",
    "\n",
    "Here's the code in one big chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def get_and_clean_penguin_data() -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Get and clean ``Penguins`` data.\n",
    "\n",
    "    Loads ``Penguins`` data, removes rows with null values for ``sex``.\n",
    "    Returns (df_features, df_target) as a tuple of dataframes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pd.DataFrame, pd.DataFrame]\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(sns.load_dataset(\"penguins\"))  # type: ignore\n",
    "    df.dropna(subset=[\"sex\"], inplace=True)\n",
    "\n",
    "    # Transform Male/Female into 0/1.\n",
    "    targets: pd.Series = df[\"sex\"].apply(\n",
    "        lambda x: 0 if x == \"Male\" else 1\n",
    "    )  # type: ignore\n",
    "\n",
    "    return (df.drop(\"sex\", axis=1), targets)\n",
    "\n",
    "\n",
    "# PREPROCESSING PIPELINES\n",
    "pipeline_numeric = Pipeline(\n",
    "    [\n",
    "        (\"impute_w_mean\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scale_normal\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_categorical = Pipeline(\n",
    "    [\n",
    "        (\"impute_w_most_frequent\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"one_hot_encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "categorical_cols = [\"species\", \"island\"]\n",
    "\n",
    "preprocessing_transformer = ColumnTransformer(\n",
    "    [\n",
    "        (\"numeric\", pipeline_numeric, numeric_cols),\n",
    "        (\"categorical\", pipeline_categorical, categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# MODEL PIPELINES\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "preprocess_model_pipeline = Pipeline(\n",
    "    [(\"preprocessing\", preprocessing_transformer), (\"random_forest_classifier\", rf_clf)]\n",
    ")\n",
    "\n",
    "# TRAINING AND SCORING\n",
    "df_features, df_target = get_and_clean_penguin_data()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_features, df_target, test_size=0.33, random_state=1234\n",
    ")\n",
    "\n",
    "pmp = preprocess_model_pipeline.fit(x_train, y_train)\n",
    "y_predicted = pmp.predict(x_test)\n",
    "\n",
    "scores = np.array(\n",
    "    [\n",
    "        (\"accuracy\", accuracy_score(y_test, y_predicted)),\n",
    "        (\"precision\", precision_score(y_test, y_predicted)),\n",
    "        (\"recall\", recall_score(y_test, y_predicted)),\n",
    "        (\"f1\", f1_score(y_test, y_predicted)),\n",
    "    ]\n",
    ")\n",
    "df_scores = pd.DataFrame(scores[:, 1], index=scores[:, 0], columns=[\"value\"])\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, with this structure, we could make different pipeline \"pieces\" to try out different classifiers, different params, etc.  The code is still a bit messy but for EDA it's able to be read through easily and able to be modified as needed with minimal difficulty."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "409c0ef82ba8b7e64c8e82f82fa10040b189117e3f7cf50921b9ec62b5d5a915"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('blog-Mz1OaUtd-py3.9': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
